{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM heartrateclass prediction\n",
    "\n",
    "Inspiration taken from: https://github.com/rikluost/athlete_hr_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import os, glob \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# location of the fit files\n",
    "fit_path = \"../fit_file_csv\"\n",
    "fit_test_path = \"../fit_file_test_csv\"\n",
    "graph_path = \"../graphs\"\n",
    "os.chdir(fit_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add calculated altitude difference column, and 5sec moving average column. Remove geographical coordinates for privacy.\n",
    "fit_files = glob.glob(\"*.csv\")\n",
    "for file in fit_files:\n",
    "    df = pd.read_csv(fit_path+'/'+file, index_col='timestamp')\n",
    "    df['alt_difference'] = df['enhanced_altitude'] - df['enhanced_altitude'].shift(1)\n",
    "    df['rolling_ave_alt'] = df['alt_difference'].rolling(window=5).mean()\n",
    "    df = df.bfill()\n",
    "    df = df.drop(['position_lat','position_long'], axis=1, errors='ignore')\n",
    "    df.to_csv(fit_path+'/'+file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the entire CSV file\n",
    "data = pd.read_csv('../with_ranges_features.csv')\n",
    "\n",
    "# Determine the number of classes and create a global label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['HeartRateClass'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Group by 'RunID'\n",
    "grouped = data.groupby('RunID')\n",
    "\n",
    "def preprocess(df):\n",
    "    # Extract features and label\n",
    "    features = df[['Latitude', 'Longitude', 'Elevation', 'Distance', 'HeartRate', 'Cadence', 'Speed']]\n",
    "    label = df['HeartRateClass']\n",
    "    \n",
    "    # Encode labels as integers and then convert to categorical\n",
    "    label = label_encoder.transform(label)\n",
    "    label = to_categorical(label, num_classes=num_classes)\n",
    "    \n",
    "    return features, label\n",
    "\n",
    "def split_data(df):\n",
    "    features, label = preprocess(df)\n",
    "    # Split into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(features, label, test_size=0.2, random_state=42)\n",
    "    return x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(7,)),  # 7 features\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')  # Multi-class classification\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RunID: 741590575\n",
      "Epoch 1/50\n",
      "\u001b[1m33/60\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3354 - loss: 41.5474      \n",
      "Epoch 1: val_loss improved from inf to 10.08746, saving model to model_checkpoint_741590575.weights.h5\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.3672 - loss: 31.4918 - val_accuracy: 0.4614 - val_loss: 10.0875\n",
      "Epoch 2/50\n",
      "\u001b[1m35/60\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4875 - loss: 6.7425 \n",
      "Epoch 2: val_loss improved from 10.08746 to 2.03569, saving model to model_checkpoint_741590575.weights.h5\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5195 - loss: 6.3255 - val_accuracy: 0.6806 - val_loss: 2.0357\n",
      "Epoch 3/50\n",
      "\u001b[1m39/60\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6601 - loss: 4.2249 \n",
      "Epoch 3: val_loss did not improve from 2.03569\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6866 - loss: 4.0232 - val_accuracy: 0.7516 - val_loss: 3.0318\n",
      "Epoch 4/50\n",
      "\u001b[1m37/60\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7557 - loss: 3.9053 \n",
      "Epoch 4: val_loss did not improve from 2.03569\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7652 - loss: 3.8555 - val_accuracy: 0.7474 - val_loss: 4.1323\n",
      "Epoch 5/50\n",
      "\u001b[1m36/60\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7473 - loss: 5.6991 \n",
      "Epoch 5: val_loss improved from 2.03569 to 0.98386, saving model to model_checkpoint_741590575.weights.h5\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7640 - loss: 5.6657 - val_accuracy: 0.9019 - val_loss: 0.9839\n",
      "Epoch 6/50\n",
      "\u001b[1m36/60\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8304 - loss: 1.7375 \n",
      "Epoch 6: val_loss did not improve from 0.98386\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8227 - loss: 2.1339 - val_accuracy: 0.5658 - val_loss: 11.4202\n",
      "Epoch 7/50\n",
      "\u001b[1m34/60\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7614 - loss: 6.1778  \n",
      "Epoch 7: val_loss did not improve from 0.98386\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7532 - loss: 6.0797 - val_accuracy: 0.7495 - val_loss: 7.0920\n",
      "Epoch 8/50\n",
      "\u001b[1m36/60\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7560 - loss: 4.9135 \n",
      "Epoch 8: val_loss did not improve from 0.98386\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7386 - loss: 5.6600 - val_accuracy: 0.8956 - val_loss: 5.1168\n",
      "Epoch 9/50\n",
      "\u001b[1m39/60\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8719 - loss: 3.3112 \n",
      "Epoch 9: val_loss did not improve from 0.98386\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8634 - loss: 3.3676 - val_accuracy: 0.9228 - val_loss: 1.3553\n",
      "Epoch 10/50\n",
      "\u001b[1m32/60\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8752 - loss: 1.0652 \n",
      "Epoch 10: val_loss did not improve from 0.98386\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8718 - loss: 1.3602 - val_accuracy: 0.9186 - val_loss: 1.1913\n",
      "Epoch 10: early stopping\n",
      "Processing RunID: 744634852\n",
      "Epoch 1/50\n",
      "\u001b[1m 87/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5165 - loss: 138.4185\n",
      "Epoch 1: val_loss improved from inf to 3.34543, saving model to model_checkpoint_744634852.weights.h5\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5096 - loss: 123.7052 - val_accuracy: 0.6307 - val_loss: 3.3454\n",
      "Epoch 2/50\n",
      "\u001b[1m 74/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6054 - loss: 9.2860 \n",
      "Epoch 2: val_loss improved from 3.34543 to 1.88648, saving model to model_checkpoint_744634852.weights.h5\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6148 - loss: 8.5928 - val_accuracy: 0.7398 - val_loss: 1.8865\n",
      "Epoch 3/50\n",
      "\u001b[1m 78/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6995 - loss: 3.1774\n",
      "Epoch 3: val_loss did not improve from 1.88648\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6927 - loss: 3.4129 - val_accuracy: 0.7278 - val_loss: 2.0196\n",
      "Epoch 4/50\n",
      "\u001b[1m 77/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6566 - loss: 3.2168\n",
      "Epoch 4: val_loss did not improve from 1.88648\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6576 - loss: 3.5480 - val_accuracy: 0.6918 - val_loss: 2.3449\n",
      "Epoch 5/50\n",
      "\u001b[1m 74/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6935 - loss: 3.6195\n",
      "Epoch 5: val_loss improved from 1.88648 to 1.22663, saving model to model_checkpoint_744634852.weights.h5\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6873 - loss: 3.8898 - val_accuracy: 0.7230 - val_loss: 1.2266\n",
      "Epoch 6/50\n",
      "\u001b[1m 72/105\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6851 - loss: 2.9043\n",
      "Epoch 6: val_loss did not improve from 1.22663\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6832 - loss: 2.9282 - val_accuracy: 0.7350 - val_loss: 2.1082\n",
      "Epoch 7/50\n",
      "\u001b[1m 77/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6712 - loss: 4.1393\n",
      "Epoch 7: val_loss did not improve from 1.22663\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6736 - loss: 4.0095 - val_accuracy: 0.6894 - val_loss: 1.4941\n",
      "Epoch 8/50\n",
      "\u001b[1m 73/105\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6603 - loss: 7.7964\n",
      "Epoch 8: val_loss did not improve from 1.22663\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6668 - loss: 7.2596 - val_accuracy: 0.5923 - val_loss: 8.1942\n",
      "Epoch 9/50\n",
      "\u001b[1m101/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6565 - loss: 6.5105\n",
      "Epoch 9: val_loss did not improve from 1.22663\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6581 - loss: 6.4093 - val_accuracy: 0.7266 - val_loss: 6.7657\n",
      "Epoch 10/50\n",
      "\u001b[1m 73/105\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6676 - loss: 7.1645\n",
      "Epoch 10: val_loss did not improve from 1.22663\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6642 - loss: 7.2950 - val_accuracy: 0.7290 - val_loss: 6.0562\n",
      "Epoch 10: early stopping\n",
      "Processing RunID: 749703915\n",
      "Epoch 1/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 841ms/step - accuracy: 0.0000e+00 - loss: 381.5092\n",
      "Epoch 1: val_loss improved from inf to 22.34422, saving model to model_checkpoint_749703915.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.0276 - loss: 247.5172 - val_accuracy: 0.3677 - val_loss: 22.3442\n",
      "Epoch 2/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3750 - loss: 28.1363\n",
      "Epoch 2: val_loss improved from 22.34422 to 4.93219, saving model to model_checkpoint_749703915.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3949 - loss: 13.6994 - val_accuracy: 0.5022 - val_loss: 4.9322\n",
      "Epoch 3/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6562 - loss: 3.9384\n",
      "Epoch 3: val_loss improved from 4.93219 to 3.30348, saving model to model_checkpoint_749703915.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4345 - loss: 4.0550 - val_accuracy: 0.4798 - val_loss: 3.3035\n",
      "Epoch 4/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5000 - loss: 3.1962\n",
      "Epoch 4: val_loss improved from 3.30348 to 2.52669, saving model to model_checkpoint_749703915.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4697 - loss: 2.8105 - val_accuracy: 0.5336 - val_loss: 2.5267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4688 - loss: 3.4568\n",
      "Epoch 5: val_loss improved from 2.52669 to 2.32191, saving model to model_checkpoint_749703915.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4549 - loss: 2.8931 - val_accuracy: 0.5874 - val_loss: 2.3219\n",
      "Epoch 6/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4375 - loss: 2.8777\n",
      "Epoch 6: val_loss improved from 2.32191 to 1.75156, saving model to model_checkpoint_749703915.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5113 - loss: 2.6538 - val_accuracy: 0.5695 - val_loss: 1.7516\n",
      "Epoch 7/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5625 - loss: 1.3863\n",
      "Epoch 7: val_loss did not improve from 1.75156\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4506 - loss: 4.1196 - val_accuracy: 0.4395 - val_loss: 5.6210\n",
      "Epoch 8/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4688 - loss: 5.3748\n",
      "Epoch 8: val_loss did not improve from 1.75156\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4589 - loss: 4.7901 - val_accuracy: 0.5874 - val_loss: 2.3084\n",
      "Epoch 9/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5938 - loss: 2.0964\n",
      "Epoch 9: val_loss did not improve from 1.75156\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4740 - loss: 2.3257 - val_accuracy: 0.5471 - val_loss: 3.1403\n",
      "Epoch 10/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5000 - loss: 4.4806\n",
      "Epoch 10: val_loss did not improve from 1.75156\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5164 - loss: 2.4433 - val_accuracy: 0.4888 - val_loss: 1.7927\n",
      "Epoch 11/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5312 - loss: 1.7228\n",
      "Epoch 11: val_loss did not improve from 1.75156\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5693 - loss: 1.5043 - val_accuracy: 0.6143 - val_loss: 1.8280\n",
      "Epoch 11: early stopping\n",
      "Processing RunID: 749703916\n",
      "Epoch 1/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 816ms/step - accuracy: 0.5625 - loss: 321.4870\n",
      "Epoch 1: val_loss improved from inf to 71.53822, saving model to model_checkpoint_749703916.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3838 - loss: 232.6612 - val_accuracy: 0.3587 - val_loss: 71.5382\n",
      "Epoch 2/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2500 - loss: 72.2832\n",
      "Epoch 2: val_loss improved from 71.53822 to 8.33128, saving model to model_checkpoint_749703916.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3000 - loss: 33.9002 - val_accuracy: 0.3363 - val_loss: 8.3313\n",
      "Epoch 3/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3125 - loss: 8.5823\n",
      "Epoch 3: val_loss improved from 8.33128 to 2.96531, saving model to model_checkpoint_749703916.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4389 - loss: 6.7603 - val_accuracy: 0.3632 - val_loss: 2.9653\n",
      "Epoch 4/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4062 - loss: 2.3870\n",
      "Epoch 4: val_loss improved from 2.96531 to 2.06530, saving model to model_checkpoint_749703916.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4577 - loss: 2.8324 - val_accuracy: 0.3857 - val_loss: 2.0653\n",
      "Epoch 5/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3438 - loss: 2.6190\n",
      "Epoch 5: val_loss improved from 2.06530 to 1.43354, saving model to model_checkpoint_749703916.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4957 - loss: 2.3480 - val_accuracy: 0.6278 - val_loss: 1.4335\n",
      "Epoch 6/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5625 - loss: 1.8472\n",
      "Epoch 6: val_loss did not improve from 1.43354\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5838 - loss: 1.5554 - val_accuracy: 0.6502 - val_loss: 2.0410\n",
      "Epoch 7/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5938 - loss: 2.2650\n",
      "Epoch 7: val_loss improved from 1.43354 to 1.25742, saving model to model_checkpoint_749703916.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6524 - loss: 1.5521 - val_accuracy: 0.6771 - val_loss: 1.2574\n",
      "Epoch 8/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7812 - loss: 0.9057\n",
      "Epoch 8: val_loss improved from 1.25742 to 1.08691, saving model to model_checkpoint_749703916.weights.h5\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6537 - loss: 1.3454 - val_accuracy: 0.7758 - val_loss: 1.0869\n",
      "Epoch 9/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6875 - loss: 1.6479\n",
      "Epoch 9: val_loss did not improve from 1.08691\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6727 - loss: 1.3425 - val_accuracy: 0.7623 - val_loss: 1.4278\n",
      "Epoch 10/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.8125 - loss: 0.8695\n",
      "Epoch 10: val_loss did not improve from 1.08691\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7015 - loss: 1.6549 - val_accuracy: 0.5516 - val_loss: 1.3520\n",
      "Epoch 11/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.4062 - loss: 1.3421\n",
      "Epoch 11: val_loss did not improve from 1.08691\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6261 - loss: 1.5387 - val_accuracy: 0.4529 - val_loss: 2.1890\n",
      "Epoch 12/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5625 - loss: 1.8384\n",
      "Epoch 12: val_loss did not improve from 1.08691\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5760 - loss: 1.8715 - val_accuracy: 0.6547 - val_loss: 2.1467\n",
      "Epoch 13/50\n",
      "\u001b[1m 1/28\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6562 - loss: 1.9654\n",
      "Epoch 13: val_loss did not improve from 1.08691\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6771 - loss: 1.4447 - val_accuracy: 0.6996 - val_loss: 2.2882\n",
      "Epoch 13: early stopping\n",
      "Processing RunID: 753631318\n",
      "Epoch 1/50\n",
      "\u001b[1m105/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5361 - loss: 283.6203     \n",
      "Epoch 1: val_loss improved from inf to 4.17862, saving model to model_checkpoint_753631318.weights.h5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5544 - loss: 254.7694 - val_accuracy: 0.8420 - val_loss: 4.1786\n",
      "Epoch 2/50\n",
      "\u001b[1m 99/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7566 - loss: 7.8902\n",
      "Epoch 2: val_loss did not improve from 4.17862\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7558 - loss: 7.9754 - val_accuracy: 0.7992 - val_loss: 8.9289\n",
      "Epoch 3/50\n",
      "\u001b[1m102/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7496 - loss: 6.6492\n",
      "Epoch 3: val_loss did not improve from 4.17862\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7494 - loss: 6.9191 - val_accuracy: 0.3904 - val_loss: 9.7508\n",
      "Epoch 4/50\n",
      "\u001b[1m113/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7451 - loss: 10.8669\n",
      "Epoch 4: val_loss did not improve from 4.17862\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7462 - loss: 10.9016 - val_accuracy: 0.3874 - val_loss: 21.1157\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7070 - loss: 10.9603\n",
      "Epoch 5: val_loss did not improve from 4.17862\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7122 - loss: 10.6445 - val_accuracy: 0.7452 - val_loss: 28.6591\n",
      "Epoch 6/50\n",
      "\u001b[1m101/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7413 - loss: 15.6466\n",
      "Epoch 6: val_loss did not improve from 4.17862\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7423 - loss: 15.2669 - val_accuracy: 0.5749 - val_loss: 6.9426\n",
      "Epoch 6: early stopping\n",
      "Processing RunID: 760382662\n",
      "Epoch 1/50\n",
      "\u001b[1m36/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6950 - loss: 28.8383       \n",
      "Epoch 1: val_loss improved from inf to 4.29728, saving model to model_checkpoint_760382662.weights.h5\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7180 - loss: 24.5149 - val_accuracy: 0.8345 - val_loss: 4.2973\n",
      "Epoch 2/50\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8253 - loss: 10.7486 \n",
      "Epoch 2: val_loss improved from 4.29728 to 4.10887, saving model to model_checkpoint_760382662.weights.h5\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8273 - loss: 10.6781 - val_accuracy: 0.8851 - val_loss: 4.1089\n",
      "Epoch 3/50\n",
      "\u001b[1m31/55\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8871 - loss: 6.9142 \n",
      "Epoch 3: val_loss did not improve from 4.10887\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8780 - loss: 7.5056 - val_accuracy: 0.6759 - val_loss: 4.4399\n",
      "Epoch 4/50\n",
      "\u001b[1m35/55\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8062 - loss: 9.1368 \n",
      "Epoch 4: val_loss did not improve from 4.10887\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8221 - loss: 9.3002 - val_accuracy: 0.9057 - val_loss: 4.7177\n",
      "Epoch 5/50\n",
      "\u001b[1m38/55\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8740 - loss: 4.4942 \n",
      "Epoch 5: val_loss did not improve from 4.10887\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8702 - loss: 4.9032 - val_accuracy: 0.8943 - val_loss: 11.2069\n",
      "Epoch 6/50\n",
      "\u001b[1m39/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8656 - loss: 6.2393 "
     ]
    }
   ],
   "source": [
    "# Iterate over each group\n",
    "for run_id, group in grouped:\n",
    "    print(f\"Processing RunID: {run_id}\")\n",
    "    \n",
    "    df = group.copy()\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = split_data(df)\n",
    "\n",
    "    # Define callbacks\n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5, verbose=1)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'./logs/run_{run_id}', histogram_freq=1)\n",
    "    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=f'model_checkpoint_{run_id}.weights.h5',\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = build_model(num_classes)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=50,\n",
    "        callbacks=[es_callback, tensorboard_callback, modelckpt_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load entire CSV file\n",
    "data = pd.read_csv('../with_ranges_features.csv')\n",
    "\n",
    "# ensure that run sessions are grouped\n",
    "grouped = data.groupby('RunID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000002230E7B9640>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Extract features and label\n",
    "    features = df[['Latitude', 'Longitude', 'Elevation', 'Distance', 'HeartRate', 'Cadence', 'Speed']]\n",
    "    label = df['HeartRateClass']\n",
    "    \n",
    "    # Encode labels as integers and then convert to categorical\n",
    "    label_encoder = LabelEncoder()\n",
    "    label = label_encoder.fit_transform(label)\n",
    "    label = to_categorical(label)\n",
    "    \n",
    "    return features, label\n",
    "\n",
    "def split_data(df):\n",
    "    features, label = preprocess(df)\n",
    "    # Split into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = train_test_split(features, label, test_size=0.2, random_state=42)\n",
    "    return x_train, x_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(7,)),  # 7 features\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(32, activation='relu'),\n",
    "        keras.layers.Dense(num_classes, activation='softmax')  # Multi-class classification\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RunID: 741590575\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 5), output.shape=(None, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m], loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelckpt_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:554\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape):\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n\u001b[1;32m--> 554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArguments `target` and `output` must have the same shape. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    557\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         )\n\u001b[0;32m    560\u001b[0m output, from_logits \u001b[38;5;241m=\u001b[39m _get_logits(\n\u001b[0;32m    561\u001b[0m     output, from_logits, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    562\u001b[0m )\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_logits:\n",
      "\u001b[1;31mValueError\u001b[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 5), output.shape=(None, 9)"
     ]
    }
   ],
   "source": [
    "# Determine the number of classes\n",
    "num_classes = len(data['HeartRateClass'].unique())\n",
    "\n",
    "# Iterate over each group\n",
    "for run_id, group in grouped:\n",
    "    print(f\"Processing RunID: {run_id}\")\n",
    "    \n",
    "    df = group.copy()\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    x_train, x_val, y_train, y_val = split_data(df)\n",
    "\n",
    "    # Define callbacks\n",
    "    es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5, verbose=1)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=f'./logs/run_{run_id}', histogram_freq=1)\n",
    "    modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=f'model_checkpoint_{run_id}.weights.h5',\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = build_model(num_classes)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=50,\n",
    "        callbacks=[es_callback, tensorboard_callback, modelckpt_callback]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters, file names\n",
    "\n",
    "#select the predictors for the model:\n",
    "model_features =  [\"heart_rate\", \"enhanced_speed\",\"rolling_ave_alt\",\"cadence\"] #  cadence, altitude, distance, heart_rate, enhanced_speed, rolling_ave_alt\n",
    "batch_size = 250 # training batch size for the LSTM\n",
    "epochs = 5 # maximum number of epochs - autostop will work on per file basis\n",
    "learning_rate = 0.001\n",
    "decay_rate = 0.001\n",
    "n_X = 120 # number of timesteps for training\n",
    "n_y = 22 # number of timesteps in future for prediction\n",
    "step = 1 # step size of predictors for model training\n",
    "\n",
    "sequence_length = int(n_X/step)\n",
    "n_fit_files_test_set = 10 # number of files for validation dataset (only 1 validation file supported at the moment)\n",
    "\n",
    "# select the training files and the validation files \n",
    "train_files = glob.glob(fit_path+\"/*.csv\")[0:-n_fit_files_test_set]\n",
    "valid_files = glob.glob(fit_path+\"/*.csv\")[-n_fit_files_test_set:]\n",
    "test_files = glob.glob(fit_test_path+\"/*.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the data normalisation parameters from all training data\n",
    "\n",
    "def normalize(data):\n",
    "    data_mean = data.mean(axis=0)\n",
    "    data_std = data.std(axis=0)\n",
    "    #return (data - data_mean) / data_std, data_mean, data_std\n",
    "    return data_mean, data_std\n",
    "\n",
    "li = []\n",
    "\n",
    "for file in train_files:\n",
    "    df = pd.read_csv(file, index_col='timestamp')[model_features]\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "df_mean, df_std = normalize(df)\n",
    "\n",
    "def denormalize_hr(data):\n",
    "    return data*df_std[0]+df_mean[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset - train each file at the time\n",
    "n=0\n",
    "for file in valid_files:\n",
    "    df = pd.read_csv(file, index_col='timestamp')[model_features]\n",
    "    df = (df - df_mean) / df_std\n",
    "    start = n_X + n_y\n",
    "    end = n_X + len(df.index)\n",
    "    \n",
    "    x = df[model_features].values\n",
    "    y = df.iloc[start:end][[\"heart_rate\"]]\n",
    "    \n",
    "    dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x,\n",
    "        y,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    if n==0 : dataset_val_old = dataset_val\n",
    "    if n>0 : dataset_val_old = dataset_val.concatenate(dataset_val_old)\n",
    "    \n",
    "    n=n+1\n",
    "\n",
    "dataset_val = dataset_val_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate stats for a naive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for the naive model\n",
    "# make dataframe for the naive model\n",
    "d_naive = pd.DataFrame(columns=['measured', 'predicted'])\n",
    "d_naive['measured']=denormalize_hr(x[n_y:,0])\n",
    "d_naive['predicted']=denormalize_hr(x[:-n_y,0])\n",
    "\n",
    "# calculate some stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy\n",
    "\n",
    "y_test, pred_test = d_naive['measured'].values, d_naive['predicted'].values\n",
    "\n",
    "MSE_test=round(mean_squared_error(y_test, pred_test, squared=True),3)\n",
    "MAE_test=round(mean_absolute_error(y_test, pred_test),3)\n",
    "\n",
    "test_sdev = np.std(pred_test-y_test)*1.96\n",
    "test_mean = np.mean(pred_test-y_test)\n",
    "\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h, h\n",
    "\n",
    "mean_s, ci95_l, ci95_h, mean_uncertainty = mean_confidence_interval(data=(pred_test-y_test))\n",
    "\n",
    "print('Naive model\\nMAE = '+ str(MAE_test)+\", MSE = \"+str(MSE_test))\n",
    "print ('Mean and 95% prediction interval = {} +/- {}'.format(test_mean,test_sdev))\n",
    "print('Uncertainty of mean = '+ str(mean_uncertainty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "# get the shapes of X & y for a batch\n",
    "for batch in dataset_val.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "# the model architecture\n",
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "outputs = keras.layers.LSTM(4, return_sequences=False)(inputs)\n",
    "outputs = keras.layers.Dense(1)(outputs)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# learning rate\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=50000,\n",
    "    decay_rate=0.001)\n",
    "\n",
    "path_checkpoint = \"model_checkpoint.weights.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_mae\", min_delta=0, patience=5, verbose=1)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs/', histogram_freq=1)\n",
    "\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_mae\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), metrics=[\"mae\"], loss=\"mae\")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data, train the model\n",
    "\n",
    "Each file is processed separately for creating the training dataset, as every file is disconnected from previous file and the moving window cannot be extendented over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset\n",
    "\n",
    "n=0\n",
    "for file in train_files:\n",
    "    df = pd.read_csv(file, index_col='timestamp')[model_features]\n",
    "    df = (df - df_mean) / df_std\n",
    "    print(file)\n",
    "    start = n_X + n_y\n",
    "    end = n_X + len(df.index)\n",
    "    \n",
    "    x = df[model_features].values\n",
    "    y = df.iloc[start:end][[\"heart_rate\"]].values\n",
    "    \n",
    "    dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x,\n",
    "        y,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    if n==0 : dataset_train_old = dataset_train\n",
    "    if n>0 : dataset_train_old = dataset_train.concatenate(dataset_train_old)\n",
    "\n",
    "    n=n+1\n",
    "\n",
    "dataset_train=dataset_train_old\n",
    "\n",
    "len(dataset_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model, with tensorboard visualisations\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback, tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")\n",
    "plt.savefig(graph_path+'/HR_his_t'+str(n_y)+\".png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the model predictions visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "for file in test_files:\n",
    "    df = pd.read_csv(file, index_col='timestamp')[model_features]\n",
    "    df = (df - df_mean) / df_std\n",
    "    print(file)\n",
    "    start = n_X + n_y\n",
    "    end = n_X + len(df.index)\n",
    "    \n",
    "    x = df[model_features].values\n",
    "    y = df.iloc[start:end][[\"heart_rate\"]].values\n",
    "    \n",
    "    dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
    "        x,\n",
    "        y,\n",
    "        sequence_length=sequence_length,\n",
    "        sampling_rate=step,\n",
    "        batch_size=10\n",
    "    )\n",
    "    \n",
    "    if n>0:\n",
    "        dataset_test_old = dataset_train_old.concatenate(dataset_test)\n",
    "    \n",
    "    dataset_test_old = dataset_test\n",
    "    \n",
    "    n=n+1\n",
    "dataset_test = dataset_test_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.ylim(100,170)\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for x, y in dataset_test.take(5):\n",
    "    show_plot(\n",
    "        [denormalize_hr(x[0][:, 0].numpy()), denormalize_hr(y[0]), denormalize_hr( model.predict(x)[0])],\n",
    "        n_y,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b77292dc63b48aca88a1593f58a471620914a6081d352eacb6540144f7e5fab5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
